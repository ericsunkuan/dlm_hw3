Conversation: Deep Learning Music Generation Assignment
ID: 2005606e-84e3-4b89-a538-2dc594c4fbdf
Created: 2024-11-09T18:20:04.946299Z
==================================================

[2024-11-09 18:16:11] HUMAN:
Dataset Structure: directory structure is given as attached file
Available GPU memory : 15GB
Any specific version requirements for libraries (pytorch, miditoolkit, etc.) : No, but I would prefer to use python 3.8, so put this in consideration
Does the TA provide the exact implementation of evaluation metrics (H4, GS) : as file attached 
How should the output files be organized for evaluation? : just keep it in a clean and organized file structure, we can format it later

Give me some more detailed plans and feedback before the code implementation

Attachments:
- eval_metrics.py
- Pop1KZ dataset structure.txt

[2024-11-09 18:03:38] HUMAN:
Please help me with this deep learning application in Music assignment. This is the instructions of this assignment and a tutorial .ipynb provided by the T.A. Please refer to these instuctions and tutorial, and implement the code for me. Please first give me an overall plan on how you are going to design the code (please use 1-stage generation) (please implement both training code for task1, inference code for task1 and task 2, optional tasks can be temporary put aside), then I will prompt you to give me the code.

[2024-11-09 18:03:38] ASSISTANT:
 I'll help you design a plan for implementing the symbolic music generation system based on the assignment requirements. Let's break this down into key components:

Overall Architecture Plan:
1. Data Processing Pipeline
- Use REMI representation as shown in the tutorial
- Process MIDI files into tokenized sequences
- Create a custom dataset class for efficient loading

2. Model Architecture (1-stage Generation)
- Use TransformerXL as the base model (as suggested in the tutorial)
- Target sequence length: 512 tokens (as recommended for limited GPU resources)
- 12 layers minimum (as per requirements)

3. Training Pipeline
- Train on the Pop1K7 dataset
- Implement different inference configurations for comparison
- Save checkpoints and training metrics

4. Inference Pipeline
Task 1:
- Generate 32 bars of music from random prompts
- Implement different sampling strategies (top-k, temperature)
- Generate both MIDI and WAV outputs

Task 2:
- Take 8-bar prompts and generate 24-bar continuations
- Use the same model but with prompt conditioning
- Generate both MIDI and WAV outputs

Implementation Structure:
```
src/
├── data/
│   ├── dataset.py        # Dataset class and data loading
│   └── preprocessing.py  # MIDI to token conversion
├── model/
│   ├── model.py         # TransformerXL implementation
│   └── config.py        # Model configuration
├── utils/
│   ├── midi_utils.py    # MIDI processing utilities
│   └── metrics.py       # Evaluation metrics
├── train.py             # Training script
└── inference.py         # Inference for both tasks
```

Key Features to Implement:
1. REMI token representation following the tutorial
2. TransformerXL model with necessary modifications for music generation
3. Training with different sampling strategies
4. Inference pipeline for both free generation and continuation
5. Evaluation metrics calculation (H4, GS)

Would you like me to start providing the code for any specific component? We can begin with either:
1. The data processing pipeline
2. The model architecture
3. The training script
4. The inference pipeline

Let me know which part you'd like to focus on first!

[2024-11-09 18:07:53] HUMAN:
ok, I have some questions before you implement the code. First, when is this model "TransformerXL" first used on music generation task and how well is it compared to the SOTA methods (the time of your latest training data). Second, do you need any additional information, such as how the dataset directory is stuctured, or the annotation file or anything else ?

[2024-11-09 18:07:53] ASSISTANT:
 Let me address your questions:

1. TransformerXL in Music Generation:
TransformerXL was first introduced by Dai et al. in 2019 for language modeling, and its application to music generation became prominent with the Pop Music Transformer paper (Huang & Yang, 2020) which you might notice is referenced in the homework PDF. While it's not the absolute SOTA as of my last training (more recent approaches like MusicLM or MT3 have shown better results), it remains a solid baseline for this task because:
- It handles long-range dependencies well through its segment-level recurrence mechanism
- It's relatively efficient to train compared to larger models
- It's particularly suitable for sequential generation tasks

As of April 2024, more advanced models exist, but for this educational assignment, TransformerXL provides a good balance of performance and complexity.

2. Additional Information Needed:
Yes, I would need some important information:

a) Dataset Structure:
- How is the Pop1K7 dataset organized? (directory structure)
- Are the MIDI files all in one folder or organized in subfolders?
- Typical naming convention of the files?

b) File Formats:
- Are all files in .mid format or do some use .midi extension?
- Are there any accompanying metadata files?

c) System Requirements:
- Available GPU memory (to optimize batch size and sequence length)
- Any specific version requirements for libraries (pytorch, miditoolkit, etc.)

d) Evaluation:
- Does the TA provide the exact implementation of evaluation metrics (H4, GS)?
- How should the output files be organized for evaluation?

Could you provide these details? This would help me design a more specific and efficient implementation that matches your setup.

Also, one important technical detail: The tutorial shows usage of miditoolkit for MIDI processing. Should we stick with that or would you prefer using pretty_midi or midiTok which might have more features but would require adapting the tutorial code?

[2024-11-09 18:16:11] ASSISTANT:
 Thank you for providing these details. Let me break down the implementation plan and provide some key insights:

1. Dataset Structure Analysis:
- The Pop1K7 dataset is organized in multiple directories (corpus, midi_analyzed, midi_synchronized, midi_transcribed)
- All MIDI files are numbered systematically (e.g., 489.mid to 1743.mid)
- Based on the Hugging Face tutorial in the notebook, we'll use the REMI representation as it provides good results for pop music

2. Implementation Plan:

A. Data Processing (src/data/):
```python
preprocessing.py:
- MIDI file reading (using miditoolkit)
- Event sequence generation (following the tutorial's REMI format)
- Tokenization (similar to tutorial but optimized for batch processing)
- Data augmentation techniques (pitch shifting, time stretching)

dataset.py:
- Custom PyTorch Dataset class for efficient loading
- Batch collation with padding
- Train/validation split functionality
```

B. Model Architecture (src/model/):
```python
config.py:
- Model hyperparameters
- Training configurations
- Data configurations

model.py:
- TransformerXL implementation 
- Modified attention mechanism for music
- Position encoding optimized for REMI tokens
```

C. Training (src/train/):
```python
train.py:
- Main training loop
- Logging & checkpointing
- Validation
- Different sampling strategies implementation

metrics.py:
- Implementation of H4 and GS metrics
- Real-time monitoring during training
```

D. Generation (src/generate/):
```python
generate.py:
- Free generation (Task 1)
- Continuation generation (Task 2)
- Different sampling strategies (temperature, top-k, etc.)
- MIDI file creation from generated sequences
```

3. Key Technical Considerations:

a) Memory Usage:
- With 15GB GPU memory, we can use:
  - Batch size: 8-16 (to be tuned)
  - Sequence length: 512 (as per tutorial recommendation)
  - Model size: 12-16 layers (around 100M parameters)

b) Performance Optimizations:
- Gradient checkpointing for memory efficiency
- Mixed precision training (fp16)
- Efficient data loading with pinned memory

c) Generation Strategies:
- Task 1: Multiple sampling temperatures (0.8, 1.0, 1.2)
- Task 2: More conservative sampling for continuation
- Beam search for improved quality

4. Evaluation Pipeline:
- Integration with provided eval_metrics.py
- Automated batch evaluation of generated samples
- Real-time metrics during training
- Result logging and visualization

5. Project Structure:
```
musicgen/
├── src/
│   ├── data/
│   │   ├── preprocessing.py
│   │   └── dataset.py
│   ├── model/
│   │   ├── config.py
│   │   └── model.py
│   ├── train/
│   │   ├── train.py
│   │   └── metrics.py
│   └── generate/
│       └── generate.py
├── configs/
│   └── model_config.yaml
├── scripts/
│   ├── prepare_data.sh
│   └── train.sh
└── requirements.txt
```

Questions before implementation:
1. Would you prefer the implementation to prioritize:
   - Code readability and maintainability
   - Maximum performance optimization
   - Easy experimentation with different configurations

2. Do you want to include additional features like:
   - Tensorboard logging
   - Experiment tracking
   - Model analysis tools

3. Would you like to integrate any specific testing framework or validation approaches?

Please let me know your preferences and any additional requirements before we start the implementation.

[2024-11-09 18:20:04] ASSISTANT:
 I'll start providing the code in a structured way. Let's begin with the core components:

1. First, let's create the requirements.txt file:

```text
numpy>=1.23.5
torch>=1.9.0
transformers>=4.34.1
miditoolkit>=1.0.0
tensorboard>=2.14.0
pyyaml>=6.0.1
tqdm>=4.66.1
```

2. Next, let's create the basic project structure and start with the config file (configs/model_config.yaml):

```yaml
# configs/model_config.yaml
model:
  n_layer: 12
  n_head: 8
  d_model: 512
  d_head: 64
  d_inner: 2048
  dropout: 0.1
  dropatt: 0.0
  mem_len: 512
  ext_len: 0
  tgt_len: 512

data:
  vocab_size: 512  # Will be updated based on actual vocab
  sequence_length: 512
  batch_size: 16
  num_workers: 4

training:
  epochs: 100
  learning_rate: 0.0001
  warmup_steps: 1000
  gradient_clip_val: 1.0
  fp16: true
  log_every_n_steps: 100
  eval_every_n_steps: 1000
  save_every_n_steps: 5000

generation:
  temperatures: [0.8, 1.0, 1.2]
  top_k: [20, 40, 60]
  num_samples: 20
  max_length: 2048  # For 32 bars
```

3. Let's start with the data preprocessing code (src/data/preprocessing.py):

```python
import os
import pickle
from typing import List, Dict, Tuple, Optional
import numpy as np
from miditoolkit.midi.parser import MidiFile
from miditoolkit.midi.containers import Note, TempoChange

class MIDIPreprocessor:
    def __init__(self, 
                 ticks_per_beat: int = 480,
                 num_velocity_bins: int = 32,
                 num_duration_bins: int = 64,
                 max_tick: int = 1920, # 4 bars
                 min_velocity: int = 0,
                 max_velocity: int = 127):
        self.ticks_per_beat = ticks_per_beat
        self.num_velocity_bins = num_velocity_bins
        self.num_duration_bins = num_duration_bins
        self.max_tick = max_tick
        
        # Create bins
        self.velocity_bins = np.linspace(min_velocity, max_velocity, num_velocity_bins + 1, dtype=int)
        self.duration_bins = np.logspace(np.log10(24), np.log10(max_tick), num_duration_bins, dtype=int)
        
        # Special tokens
        self.special_tokens = {
            'PAD': 0,
            'BOS': 1,
            'EOS': 2,
            'BAR': 3,
        }
        
        # Create vocabulary
        self.create_vocabulary()
        
    def create_vocabulary(self):
        """Create vocabulary for REMI representation"""
        vocab = {}
        idx = len(self.special_tokens)
        
        # Add position tokens (16 positions per bar)
        for pos in range(16):
            vocab[f'Position_{pos}'] = idx
            idx += 1
            
        # Add pitch tokens
        for pitch in range(128):
            vocab[f'Pitch_{pitch}'] = idx
            idx += 1
            
        # Add velocity tokens
        for vel in range(self.num_velocity_bins):
            vocab[f'Velocity_{vel}'] = idx
            idx += 1
            
        # Add duration tokens
        for dur in range(self.num_duration_bins):
            vocab[f'Duration_{dur}'] = idx
            idx += 1
            
        # Add tempo tokens
        for tempo in ['Slow', 'Mid', 'Fast']:
            vocab[f'Tempo_{tempo}'] = idx
            idx += 1
            
        self.vocab = vocab
        self.vocab_size = len(vocab) + len(self.special_tokens)
        
        # Create reverse vocabulary
        self.rev_vocab = {v: k for k, v in vocab.items()}
        for k, v in self.special_tokens.items():
            self.rev_vocab[v] = k
            
    def quantize_value(self, value: float, bins: np.ndarray) -> int:
        """Quantize a value into bins"""
        return np.digitize(value, bins) - 1
        
    def extract_notes(self, midi_obj: MidiFile) -> List[Note]:
        """Extract notes from MIDI file"""
        notes = []
        for instr in midi_obj.instruments:
            if not instr.is_drum:  # Skip drum tracks
                notes.extend(instr.notes)
        
        # Sort by start time and pitch
        notes.sort(key=lambda x: (x.start, x.pitch))
        return notes
    
    def midi_to_events(self, midi_path: str) -> List[Dict]:
        """Convert MIDI file to REMI events"""
        midi_obj = MidiFile(midi_path)
        notes = self.extract_notes(midi_obj)
        
        events = []
        current_bar = -1
        current_position = 0
        
        for note in notes:
            # Add bar token if needed
            bar_num = note.start // (self.ticks_per_beat * 4)
            if bar_num > current_bar:
                events.append({
                    'type': 'BAR',
                    'value': bar_num
                })
                current_bar = bar_num
            
            # Add position token
            position = (note.start % (self.ticks_per_beat * 4)) // (self.ticks_per_beat // 4)
            if position != current_position:
                events.append({
                    'type': 'Position',
                    'value': position
                })
                current_position = position
            
            # Add note tokens
            events.append({
                'type': 'Pitch',
                'value': note.pitch
            })
            
            velocity_idx = self.quantize_value(note.velocity, self.velocity_bins)
            events.append({
                'type': 'Velocity',
                'value': velocity_idx
            })
            
            duration = note.end - note.start
            duration_idx = self.quantize_value(duration, self.duration_bins)
            events.append({
                'type': 'Duration',
                'value': duration_idx
            })
            
        return events
    
    def events_to_tokens(self, events: List[Dict]) -> List[int]:
        """Convert events to token indices"""
        tokens = [self.special_tokens['BOS']]
        
        for event in events:
            event_type = event['type']
            value = event['value']
            
            if event_type == 'BAR':
                tokens.append(self.special_tokens['BAR'])
            else:
                token_name = f'{event_type}_{value}'
                if token_name in self.vocab:
                    tokens.append(self.vocab[token_name])
                
        tokens.append(self.special_tokens['EOS'])
        return tokens
    
    def save_vocabulary(self, path: str):
        """Save vocabulary to file"""
        with open(path, 'wb') as f:
            pickle.dump({
                'vocab': self.vocab,
                'rev_vocab': self.rev_vocab,
                'special_tokens': self.special_tokens,
                'config': {
                    'ticks_per_beat': self.ticks_per_beat,
                    'num_velocity_bins': self.num_velocity_bins,
                    'num_duration_bins': self.num_duration_bins,
                    'max_tick': self.max_tick,
                }
            }, f)
    
    @classmethod
    def load_vocabulary(cls, path: str) -> 'MIDIPreprocessor':
        """Load vocabulary from file"""
        with open(path, 'rb') as f:
            data = pickle.load(f)
        
        preprocessor = cls(**data['config'])
        preprocessor.vocab = data['vocab']
        preprocessor.rev_vocab = data['rev_vocab']
        preprocessor.special_tokens = data['special_tokens']
        preprocessor.vocab_size = len(preprocessor.vocab) + len(preprocessor.special_tokens)
        return preprocessor
```

4. Next, let's create the dataset class (src/data/dataset.py):

```python
import torch
from torch.utils.data import Dataset, DataLoader
from typing import List, Dict, Optional
import os
import pickle
from tqdm import tqdm

class MusicDataset(Dataset):
    def __init__(self, 
                 data_dir: str,
                 preprocessor: 'MIDIPreprocessor',
                 max_seq_len: int = 512,
                 cache_dir: Optional[str] = None):
        self.data_dir = data_dir
        self.preprocessor = preprocessor
        self.max_seq_len = max_seq_len
        self.cache_dir = cache_dir
        
        # Load or create cached data
        if cache_dir and os.path.exists(os.path.join(cache_dir, 'cached_data.pkl')):
            self.load_cache()
        else:
            self.sequences = self.process_all_files()
            if cache_dir:
                self.save_cache()
                
    def process_all_files(self) -> List[torch.Tensor]:
        """Process all MIDI files in directory"""
        sequences = []
        midi_files = [f for f in os.listdir(self.data_dir) if f.endswith('.mid')]
        
        for midi_file in tqdm(midi_files, desc="Processing MIDI files"):
            try:
                events = self.preprocessor.midi_to_events(
                    os.path.join(self.data_dir, midi_file)
                )
                tokens = self.preprocessor.events_to_tokens(events)
                
                # Split into sequences of max_seq_len
                for i in range(0, len(tokens) - self.max_seq_len, self.max_seq_len // 2):
                    seq = tokens[i:i + self.max_seq_len]
                    if len(seq) == self.max_seq_len:
                        sequences.append(torch.tensor(seq, dtype=torch.long))
                        
            except Exception as e:
                print(f"Error processing {midi_file}: {str(e)}")
                continue
                
        return sequences
    
    def save_cache(self):
        """Save processed data to cache"""
        if self.cache_dir:
            os.makedirs(self.cache_dir, exist_ok=True)
            with open(os.path.join(self.cache_dir, 'cached_data.pkl'), 'wb') as f:
                pickle.dump(self.sequences, f)
                
    def load_cache(self):
        """Load processed data from cache"""
        with open(os.path.join(self.cache_dir, 'cached_data.pkl'), 'rb') as f:
            self.sequences = pickle.load(f)
            
    def __len__(self) -> int:
        return len(self.sequences)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        seq = self.sequences[idx]
        
        # Create input and target sequences
        input_seq = seq[:-1]
        target_seq = seq[1:]
        
        return {
            'input_ids': input_seq,
            'labels': target_seq
        }
    
    @staticmethod
    def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """Collate function for DataLoader"""
        input_ids = torch.stack([item['input_ids'] for item in batch])
        labels = torch.stack([item['labels'] for item in batch])
        
        return {
            'input_ids': input_ids,
            'labels': labels
        }
```

This is the first part of the implementation. I'll continue with the model implementation, training, and generation code in the next message. Would you like me to continue, or would you like to review and discuss these components first?

The current code includes:
1. Project setup with requirements
2. Configuration setup
3. Data preprocessing pipeline
4. Dataset handling

The key features implemented so far:
- REMI token representation
- Efficient data processing with caching
- Configurable preprocessing parameters
- Robust error handling
- Memory-efficient data loading

Would you like me to proceed with the model implementation next?

[2024-11-09 18:20:04] HUMAN:
I would prefer to prioritize Code readability and maintainability then Easy experimentation with different configurations then Maximum performance optimization, but rather short training time would also needed to be considered 
Please include Tensorboard logging

Now please give me the code

